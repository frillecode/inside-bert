model: "distilbert-base-uncased"
task: "MNLI"

timestamp: "2024-07-26_14-03-57" # run from office, think good

output_dir: /work/Results # "/Users/au617011/Documents/Thesis-results/"

training_args:
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  num_train_epochs: 3
  logging_dir: "logs"
  logging_steps: 50
  evaluation_strategy: "steps"
  save_steps: 50